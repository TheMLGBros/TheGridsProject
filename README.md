# The Grids Project

This repository contains a small arcade-based prototype. Automated tests have been added under the `tests/` directory.

The project requires the real [`arcade`](https://api.arcade.academy/) package. Install it along with `pytest` to run the game or the unit tests.

Unit, item and UI images are stored under the `sprites/` directory. Each type has its own subfolder to keep assets organized.

## Running the Game

To start the game, run the `grids.py` script:

```bash
python grids.py
```

This launches a window where you can move units, play spell cards and end your turn using the on-screen buttons.
There are separate buttons for drawing spells and units into your hand.


## Running Tests

Install dependencies (e.g. `arcade` and `pytest`) and run:

```bash
pytest
```

This will execute all unit tests.

## Self-Play Example

The repository includes a simple `RandomAgent` and a `self_play.py` script
demonstrating how two agents can interact with the `GridsEnv` environment.
Run the script to watch a few turns of automated play:

```bash
python self_play.py
```

This uses purely random actions, but provides a starting point for more
advanced reinforcement learning experiments.

Valid actions are represented as a tuple ``(action_type, index, row, col)``.
The seven action types are:

* ``0`` – move the unit at ``index`` in ``state.units`` to ``(row, col)``.
* ``1`` – deploy the unit type at ``index`` from the player's hand onto the board.
* ``2`` – play the spell card at ``index`` targeting ``(row, col)``.
* ``3`` – end the current turn.
* ``4`` – attack the unit at ``(row, col)`` with ``state.units[index]``.
* ``5`` – draw a spell card from the deck (costs 1 action point).
* ``6`` – draw a unit card from the deck (costs 1 action point).

The environment's :meth:`valid_actions` method returns this list each step and
the ``RandomAgent`` simply chooses from it at random.

## DQN Training Example

A simple Deep Q-Network agent and training script are included for
experimenting with learning in `GridsEnv`. Install PyTorch and run
`train_dqn.py` to train two agents in self‑play. The weights of the first
agent are written to `dqn_model.pth` at the end of training:

```bash
pip install torch
python train_dqn.py
```

This implementation is intentionally lightweight and is aimed at CPU
training on a laptop.

During training each player is controlled by a separate ``DQNAgent``. The
agent uses an \(epsilon\)-greedy policy to pick from the valid actions returned
by the environment. Q-values are produced by a small feed-forward network with
two fully connected layers of 128 units each. After every step the transition is
stored in a replay buffer and the network is updated from a sampled batch. The
exploration rate decays from 1.0 to 0.1 across training episodes. When training
finishes the weights of ``agent1`` are saved to ``dqn_model.pth``. Once the loop
completes the script generates ``training_progress.png`` showing rewards per
episode and prints summary tables with useful and fun statistics.

## Watching AI vs AI With Graphics

To simply watch two agents play a match with the graphical interface enabled,
run the `ai_vs_ai.py` script:

```bash
python ai_vs_ai.py
```

Both agents now automatically load the weights stored in `dqn_model.pth`,
generated by `train_dqn.py`. Simply run the script to watch the trained AI
play against itself.

## Playing Against the AI

To challenge a computer controlled opponent while you take the other side,
run the `human_vs_ai.py` script:

```bash
python human_vs_ai.py
```

The script automatically loads the weights in `dqn_model.pth` so you can play
against the latest trained model without editing any code.
